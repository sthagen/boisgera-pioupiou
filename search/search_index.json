{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pioupiou \ud83d\udc24 \u00b6 Introduction \u00b6 Pioupiou is a nano probabilistic programming language, embedded into Python. Use it to define probabilistic models : import pioupiou as pp a , b = 0.5 , 1.0 X = pp . Uniform ( 0.0 , 1.0 ) E = pp . Normal ( 0.0 , 0.01 ) Y = a * X + b + E and to simulate them n = 1000 # number of samples omega = pp . Omega ( n ) x , y = X ( omega ), Y ( omega ) The results are >>> x # doctest: +ELLIPSIS array ([ 6.36961687e-01 , 2.69786714e-01 , 4.09735239e-02 , ... , 3.80007897e-01 ]) >>> y # doctest: +ELLIPSIS array ([ 1.09588258 , 1.22942954 , 1.01954509 , 0.99213115 , ... , 1.14366864 ]) That's about it! Use this data as you see fit. For example: import pandas as pd import seaborn as sns import matplotlib.pyplot as plt data = pd . DataFrame ({ \"x\" : x , \"y\" : y }) _ = sns . jointplot ( x = \"x\" , y = \"y\" , data = data , kind = \"reg\" , xlim = ( 0.0 , 1.0 ), ylim = ( 0.75 , 1.75 )) plt . savefig ( \"xy.svg\" ) Getting started \u00b6 Install the latest version of pioupiou with: $ pip install --upgrade git+https://github.com/boisgera/pioupiou.git","title":"Overview"},{"location":"#pioupiou","text":"","title":"Pioupiou \ud83d\udc24"},{"location":"#introduction","text":"Pioupiou is a nano probabilistic programming language, embedded into Python. Use it to define probabilistic models : import pioupiou as pp a , b = 0.5 , 1.0 X = pp . Uniform ( 0.0 , 1.0 ) E = pp . Normal ( 0.0 , 0.01 ) Y = a * X + b + E and to simulate them n = 1000 # number of samples omega = pp . Omega ( n ) x , y = X ( omega ), Y ( omega ) The results are >>> x # doctest: +ELLIPSIS array ([ 6.36961687e-01 , 2.69786714e-01 , 4.09735239e-02 , ... , 3.80007897e-01 ]) >>> y # doctest: +ELLIPSIS array ([ 1.09588258 , 1.22942954 , 1.01954509 , 0.99213115 , ... , 1.14366864 ]) That's about it! Use this data as you see fit. For example: import pandas as pd import seaborn as sns import matplotlib.pyplot as plt data = pd . DataFrame ({ \"x\" : x , \"y\" : y }) _ = sns . jointplot ( x = \"x\" , y = \"y\" , data = data , kind = \"reg\" , xlim = ( 0.0 , 1.0 ), ylim = ( 0.75 , 1.75 )) plt . savefig ( \"xy.svg\" )","title":"Introduction"},{"location":"#getting-started","text":"Install the latest version of pioupiou with: $ pip install --upgrade git+https://github.com/boisgera/pioupiou.git","title":"Getting started"},{"location":"calculus/","text":"Calculus \u00b6 import pioupiou as pp Custom Functions \u00b6 pp . restart () U1 = pp . Uniform () U2 = pp . Uniform ( 1.0 , 2.0 ) @pp . randomize def add ( x , y ): return x + y X = add ( U1 , U2 ) >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 2.45281524 , 1.27252521 , 1.8983778 , 1.05011321 , 2.54292569 , 2.0884112 , 2.4698147 , 2.27095778 , 1.84333688 , 2.35775964 ]) @pp . randomize def subtract ( x , y ): return x - y Z = subtract ( X , X ) >>> omega = pp . Omega ( 10 ) >>> Z ( omega ) array ([ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) >>> pp . restart () >>> print ( pp . exp ( 1.0 )) 2.718281828459045 >>> N1 = pp . Normal ( 1.0 , ( 0.001 ) ** 2 ) >>> X = pp . exp ( N1 ) >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 2.71923434 , 2.71661479 , 2.71355749 , 2.71249422 , 2.72070221 , 2.72197555 , 2.71901739 , 2.719944 , 2.71857969 , 2.72240226 ]) pp . restart () N1 = pp . Normal ( 1.0 , ( 0.01 ) ** 2 ) N2 = pp . Normal ( 2.0 , ( 0.02 ) ** 2 ) X = N1 + N2 >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 3.02149701 , 2.93831437 , 3.00397961 , 2.94207534 , 3.02113554 , 2.99493823 , 3.02459988 , 3.00819522 , 2.99059112 , 3.01124626 ]) pp . restart () U1 = pp . Uniform () X = U1 + 1.0 Y = 1.0 + U1 Z = + ( X * Y ) W = - Z >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 1.63696169 , 1.26978671 , 1.04097352 , 1.01652764 , 1.81327024 , 1.91275558 , 1.60663578 , 1.72949656 , 1.54362499 , 1.93507242 ]) >>> Y ( omega ) array ([ 1.63696169 , 1.26978671 , 1.04097352 , 1.01652764 , 1.81327024 , 1.91275558 , 1.60663578 , 1.72949656 , 1.54362499 , 1.93507242 ]) >>> Z ( omega ) array ([ 2.67964357 , 1.6123583 , 1.08362588 , 1.03332843 , 3.28794896 , 3.6586339 , 2.58127852 , 2.99115835 , 2.38277811 , 3.74450529 ]) >>> W ( omega ) array ([ - 2.67964357 , - 1.6123583 , - 1.08362588 , - 1.03332843 , - 3.28794896 , - 3.6586339 , - 2.58127852 , - 2.99115835 , - 2.38277811 , - 3.74450529 ]) >>> pp . restart () >>> U = pp . Uniform () >>> import builtins >>> V = builtins . bool ( U ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... TypeError : ... >>> V = pp . bool ( U ) pp . restart () U1 = pp . Uniform () B = ( U1 >= 1 / 3 ) >>> omega = pp . Omega ( 10 ) >>> B ( omega ) # doctest: +NORMALIZE_WHITESPACE array ([ True , False , False , False , True , True , True , True , True , True ]) pp . restart () U , C = pp . Uniform (), pp . Constant ( 1.0 ) T1 = ( U <= 1.0 ) T2 = ( 1.0 < U ) T3 = ( U <= C ) T4 = ( U < 1.0 ) T5 = ( 1.0 <= U ) >>> omega = pp . Omega () >>> T1 ( omega ) True >>> T1 ( omega ) == ( not T2 ( omega )) True >>> T1 ( omega ) == T3 ( omega ) True >>> T4 ( omega ) == ( not T5 ( omega )) True pp . restart () X , Y = pp . Uniform (), pp . Uniform () W = ( X == Y ) Z = ( X != Y ) >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ) ( 0.6369616873214543 , 0.2697867137638703 ) >>> W ( omega ), Z ( omega ) ( False , True ) >>> W ( omega ) != Z ( omega ) True Operators \u00b6 pp . restart () X , Y , Z = pp . Constant ( 1.0 ), pp . Constant ( 2.0 ), pp . Constant ( 4.0 ) >>> omega = pp . Omega () >>> ( X + Y )( omega ) 3.0 >>> ( X + 2.0 )( omega ) 3.0 >>> ( 2.0 + X )( omega ) 3.0 >>> ( X - Y )( omega ) - 1.0 >>> ( X - 2.0 )( omega ) - 1.0 >>> ( 2.0 - X )( omega ) 1.0 >>> ( Y * Z )( omega ) 8.0 >>> ( Y * 4.0 )( omega ) 8.0 >>> ( 4.0 * Y )( omega ) 8.0 >>> ( X / Y )( omega ) 0.5 >>> ( X / 2.0 )( omega ) 0.5 >>> ( 1.0 / Y )( omega ) 0.5 >>> ( pp . Constant ( 5.0 ) // pp . Constant ( 2.0 ))( omega ) 2.0 >>> ( 5.0 // pp . Constant ( 2.0 ))( omega ) 2.0 >>> ( 5.0 // pp . Constant ( 2.0 ))( omega ) 2.0","title":"Calculus"},{"location":"calculus/#calculus","text":"import pioupiou as pp","title":"Calculus"},{"location":"calculus/#custom-functions","text":"pp . restart () U1 = pp . Uniform () U2 = pp . Uniform ( 1.0 , 2.0 ) @pp . randomize def add ( x , y ): return x + y X = add ( U1 , U2 ) >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 2.45281524 , 1.27252521 , 1.8983778 , 1.05011321 , 2.54292569 , 2.0884112 , 2.4698147 , 2.27095778 , 1.84333688 , 2.35775964 ]) @pp . randomize def subtract ( x , y ): return x - y Z = subtract ( X , X ) >>> omega = pp . Omega ( 10 ) >>> Z ( omega ) array ([ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) >>> pp . restart () >>> print ( pp . exp ( 1.0 )) 2.718281828459045 >>> N1 = pp . Normal ( 1.0 , ( 0.001 ) ** 2 ) >>> X = pp . exp ( N1 ) >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 2.71923434 , 2.71661479 , 2.71355749 , 2.71249422 , 2.72070221 , 2.72197555 , 2.71901739 , 2.719944 , 2.71857969 , 2.72240226 ]) pp . restart () N1 = pp . Normal ( 1.0 , ( 0.01 ) ** 2 ) N2 = pp . Normal ( 2.0 , ( 0.02 ) ** 2 ) X = N1 + N2 >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 3.02149701 , 2.93831437 , 3.00397961 , 2.94207534 , 3.02113554 , 2.99493823 , 3.02459988 , 3.00819522 , 2.99059112 , 3.01124626 ]) pp . restart () U1 = pp . Uniform () X = U1 + 1.0 Y = 1.0 + U1 Z = + ( X * Y ) W = - Z >>> omega = pp . Omega ( 10 ) >>> X ( omega ) array ([ 1.63696169 , 1.26978671 , 1.04097352 , 1.01652764 , 1.81327024 , 1.91275558 , 1.60663578 , 1.72949656 , 1.54362499 , 1.93507242 ]) >>> Y ( omega ) array ([ 1.63696169 , 1.26978671 , 1.04097352 , 1.01652764 , 1.81327024 , 1.91275558 , 1.60663578 , 1.72949656 , 1.54362499 , 1.93507242 ]) >>> Z ( omega ) array ([ 2.67964357 , 1.6123583 , 1.08362588 , 1.03332843 , 3.28794896 , 3.6586339 , 2.58127852 , 2.99115835 , 2.38277811 , 3.74450529 ]) >>> W ( omega ) array ([ - 2.67964357 , - 1.6123583 , - 1.08362588 , - 1.03332843 , - 3.28794896 , - 3.6586339 , - 2.58127852 , - 2.99115835 , - 2.38277811 , - 3.74450529 ]) >>> pp . restart () >>> U = pp . Uniform () >>> import builtins >>> V = builtins . bool ( U ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... TypeError : ... >>> V = pp . bool ( U ) pp . restart () U1 = pp . Uniform () B = ( U1 >= 1 / 3 ) >>> omega = pp . Omega ( 10 ) >>> B ( omega ) # doctest: +NORMALIZE_WHITESPACE array ([ True , False , False , False , True , True , True , True , True , True ]) pp . restart () U , C = pp . Uniform (), pp . Constant ( 1.0 ) T1 = ( U <= 1.0 ) T2 = ( 1.0 < U ) T3 = ( U <= C ) T4 = ( U < 1.0 ) T5 = ( 1.0 <= U ) >>> omega = pp . Omega () >>> T1 ( omega ) True >>> T1 ( omega ) == ( not T2 ( omega )) True >>> T1 ( omega ) == T3 ( omega ) True >>> T4 ( omega ) == ( not T5 ( omega )) True pp . restart () X , Y = pp . Uniform (), pp . Uniform () W = ( X == Y ) Z = ( X != Y ) >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ) ( 0.6369616873214543 , 0.2697867137638703 ) >>> W ( omega ), Z ( omega ) ( False , True ) >>> W ( omega ) != Z ( omega ) True","title":"Custom Functions"},{"location":"calculus/#operators","text":"pp . restart () X , Y , Z = pp . Constant ( 1.0 ), pp . Constant ( 2.0 ), pp . Constant ( 4.0 ) >>> omega = pp . Omega () >>> ( X + Y )( omega ) 3.0 >>> ( X + 2.0 )( omega ) 3.0 >>> ( 2.0 + X )( omega ) 3.0 >>> ( X - Y )( omega ) - 1.0 >>> ( X - 2.0 )( omega ) - 1.0 >>> ( 2.0 - X )( omega ) 1.0 >>> ( Y * Z )( omega ) 8.0 >>> ( Y * 4.0 )( omega ) 8.0 >>> ( 4.0 * Y )( omega ) 8.0 >>> ( X / Y )( omega ) 0.5 >>> ( X / 2.0 )( omega ) 0.5 >>> ( 1.0 / Y )( omega ) 0.5 >>> ( pp . Constant ( 5.0 ) // pp . Constant ( 2.0 ))( omega ) 2.0 >>> ( 5.0 // pp . Constant ( 2.0 ))( omega ) 2.0 >>> ( 5.0 // pp . Constant ( 2.0 ))( omega ) 2.0","title":"Operators"},{"location":"distributions/","text":"Distributions \u00b6 import pioupiou as pp import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns To streamline the visualization of distributions in this document, we introduce a helper function long_form_data . It instantiates some distributions, simulates them and returns the results as a long-form dataframe with column names \"Distribution\" and \"Value\" . Its arguments are: distribs : strings that should eval to random variables, n : number of samples used for the simulation (default: 100000 ) def long_form_data ( * distribs , n = 100000 ): # Modeling and Simulation pp . restart () Xs = [ eval ( distrib ) for distrib in distribs ] # random variables omega = pp . Omega ( n ) xs = [ X ( omega ) for X in Xs ] # Long-form Data Frame data = [] for distrib , x in zip ( distribs , xs ): data . extend ([[ distrib , np . float64 ( value )] for value in x ]) return pd . DataFrame ( data , columns = [ \"Distribution\" , \"Value\" ]) Bernoulli \u00b6 The snippet B = pp.Bernoulli(p) instantiates a random boolean variable \\(B\\) such that \\[ \\begin{array}{lcl} \\mathbb{P}(B = \\mathrm{true}) &=& p \\\\ \\mathbb{P}(B = \\mathrm{false}) &=& 1-p \\\\ \\end{array} \\] For example: >>> pp . restart () >>> B = pp . Bernoulli ( 0.5 ) >>> omega = pp . Omega ( 10 ) >>> b = B ( omega ) >>> b # doctest: +NORMALIZE_WHITESPACE array ([ False , True , True , True , False , False , False , False , False , False ]) The parameter p is optional; its default value is 0.5 . Thus B = Bernoulli() is equivalent to B = Bernoulli(0.5) . >>> pp . restart () >>> B = pp . Bernoulli () >>> omega = pp . Omega ( 10 ) >>> all ( b == B ( omega )) True With p=0.0 or p=1.0 you will get almost surely False and True respectively. >>> B = pp . Bernoulli ( 0.0 ) >>> omega = pp . Omega ( 10 ) >>> all ( B ( omega ) == False ) True >>> B = pp . Bernoulli ( 1.0 ) >>> omega = pp . Omega ( 10 ) >>> all ( B ( omega ) == True ) True With a larger number of independent samples, we can check these probabilities in a histogram df = long_form_data ( \"pp.Bernoulli(0.0)\" , \"pp.Bernoulli(0.25)\" , \"pp.Bernoulli(0.5)\" , \"pp.Bernoulli()\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"probability\" , common_norm = False , multiple = \"dodge\" , discrete = True , shrink = 0.5 ) yticks = plt . yticks ([ 0.0 , 0.25 , 0.5 , 0.75 , 1.0 ]) xticks = plt . xticks ([ 0 , 1 ], [ \"False\" , \"True\" ]) title = plt . title ( \"Bernoulli Distribution\" ) plt . savefig ( \"bernoulli.svg\" ) plt . close () Binomial \u00b6 When \\(n \\in \\mathbb{N}\\) and \\(p \\in [0,1]\\) , the code B = pp.Binomial(n, p) instantiates a random variable \\(B\\) with probability mass function $$ f(k) = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k} $$ for \\(k \\in \\{0,n\\}\\) and \\(f(k)=0\\) otherwise. The parameter \\(p\\) has a default value of \\(0.5\\) . df = long_form_data ( \"pp.Binomial(5)\" , \"pp.Binomial(5, 0.50)\" , \"pp.Binomial(5, 0.25)\" , \"pp.Binomial(5, 0.75)\" , ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"probability\" , common_norm = False , multiple = \"dodge\" , discrete = True , shrink = 0.5 ) yticks = plt . yticks ([ 0.0 , 0.125 , 0.25 , 0.375 , 0.5 ]) title = plt . title ( \"Binomial Distribution\" ) plt . savefig ( \"binomial.svg\" ) plt . close () Poisson \u00b6 The code pp.Poisson(lambda_) creates a random variable with probability mass function $$ f(k) = \\lambda^k \\frac{e^{-\\lambda}}{k!}, \\; k \\in \\mathbb{N} $$ and \\(f(k)=0\\) otherwise. The parameter \\(\\lambda\\) should be real and positive. df = long_form_data ( \"pp.Poisson(1.0)\" , \"pp.Poisson(2.0)\" , \"pp.Poisson(4.0)\" , \"pp.Binomial(100, 0.04)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"probability\" , common_norm = False , multiple = \"dodge\" , discrete = True , shrink = 0.8 ) xlim = plt . xlim ( - 1.0 , 11.0 ) title = plt . title ( \"Poisson Distribution\" ) plt . savefig ( \"poisson.svg\" ) plt . close () Uniform \u00b6 When a < b , the snippet U = pp.Uniform(a, b) creates a random variable \\(U\\) with density $$ f(x) = \\frac{1}{b-a} \\; \\mbox{ if } \\; a \\leq x \\leq b, $$ and \\(f(x)= 0\\) otherwise. The default value of a is 0.0 and the default value of b is 1.0, thus U = pp.Uniform() is equivalent to U = pp.Uniform(0,1) . For example >>> pp . restart () >>> U = pp . Uniform () >>> omega = pp . Omega () >>> U ( omega ) 0.6369616873214543 is equivalent to >>> pp . restart () >>> U = pp . Uniform ( 0.0 , 1.0 ) >>> omega = pp . Omega () >>> U ( omega ) 0.6369616873214543 We are almost sure that values sampled from U = pp.Uniform(a, b) are between a and b : >>> pp . restart () >>> a , b = - 3 , 7 >>> U = pp . Uniform ( a , b ) >>> omega = pp . Omega ( 1000 ) >>> all ( a <= U ( omega )) and all ( U ( omega ) <= b ) True Let's visualize some examples of the uniform distribution df = long_form_data ( \"pp.Uniform(-1.5, -1.0)\" , \"pp.Uniform( 0.0, 1.0)\" , \"pp.Uniform( 2.0, 4.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , bins = np . arange ( - 2.0 , 4.5 , 0.25 ), common_norm = False , ) xticks = plt . xticks ( np . arange ( - 2.0 , 4.5 , 1.0 )) title = plt . title ( \"Uniform Distribution\" ) plt . savefig ( \"uniform.svg\" ) plt . close () Normal \u00b6 The snippet N = pp.Normal(mu, sigma**2) creates a random variable \\(N\\) with density $$ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). $$ The default values of mu and sigma**2 are 0.0 and 1.0 : >>> pp . restart () >>> N = pp . Normal () >>> omega = pp . Omega () >>> N ( omega ) 0.3503492272565639 >>> pp . restart () >>> N = pp . Normal ( 0.0 , 1.0 ) >>> omega = pp . Omega () >>> N ( omega ) 0.3503492272565639 The parameters \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma > 0\\) are the mean and standard deviation of the random variable: >>> pp . restart () >>> N = pp . Normal ( 1.0 , ( 0.1 ) ** 2 ) >>> omega = pp . Omega ( 100000 ) >>> n = N ( omega ) >>> n # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array ([ 1.03503492 , 0.93865418 , 0.82605011 , ... , 1.00156987 ]) >>> np . mean ( n ) 0.9998490788460421 >>> np . std ( n ) 0.09990891658278829 Let's visualize some normal distributions: df = long_form_data ( \"pp.Normal( 0.0, 1.0)\" , \"pp.Normal(-2.0, 1.0)\" , \"pp.Normal( 2.0, 2.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , bins = [ - 1e9 ] + list ( np . linspace ( - 5 , 5 , 10 * 5 + 1 )) + [ 1e9 ], element = \"step\" ) xlim = plt . xlim ( - 5.0 , 5.0 ) title = plt . title ( \"Normal Distribution\" ) plt . savefig ( \"normal.svg\" ) plt . close () Exponential \u00b6 >>> pp.restart() >>> E = pp.Exponential() >>> omega = pp.Omega() >>> E(omega) 1.013246905717726 >>> pp.restart() >>> E = pp.Exponential(1.0) >>> omega = pp.Omega() >>> E(omega) 1.013246905717726 >>> pp.restart() >>> E = pp.Exponential(2.0) >>> omega = pp.Omega(1000) >>> np.mean(E(omega)) 0.5170714017411246 df = long_form_data ( \"pp.Exponential(0.5)\" , \"pp.Exponential(1.0)\" , \"pp.Exponential(2.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , element = \"step\" ) xlim = plt . xlim ( 0.0 , 5.0 ) title = plt . title ( \"Exponential Distribution\" ) plt . savefig ( \"exponential.svg\" ) plt . close () Cauchy \u00b6 Cauchy(x0=0.0, gamma=1.0) generates a random variable with density $$ f(x) = \\frac{1}{\\pi \\gamma} \\frac{\\gamma^2}{(x-x_0)^2 + \\gamma^2}. $$ >>> pp.restart() >>> C = pp.Cauchy() >>> omega = pp.Omega() >>> C(omega) 0.4589573340936978 >>> pp.restart() >>> C = pp.Cauchy(0.0, 1.0) >>> omega = pp.Omega() >>> C(omega) 0.4589573340936978 >>> pp.restart() >>> C = pp.Cauchy(3.0, 2.0) >>> omega = pp.Omega(1000) >>> np.median(C(omega)) 3.181434516919701 df = long_form_data ( \"pp.Cauchy( 0.0, 1.0)\" , \"pp.Cauchy(-2.0, 1.0)\" , \"pp.Cauchy( 2.0, 2.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , bins = [ - 1e9 ] + list ( np . linspace ( - 5 , 5 , 10 * 5 + 1 )) + [ 1e9 ], element = \"step\" ) xlim = plt . xlim ( - 5.0 , 5.0 ) title = plt . title ( \"Cauchy Distribution\" ) plt . savefig ( \"cauchy.svg\" ) plt . close () Student \u00b6 df = long_form_data ( \"pp.t(0.1)\" , \"pp.t(1.0)\" , \"pp.t(10.0)\" , \"pp.Normal(0.0, 1.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , bins = [ - 1e9 ] + list ( np . linspace ( - 5 , 5 , 10 * 5 + 1 )) + [ 1e9 ], element = \"step\" , fill = False , ) xlim = plt . xlim ( - 5.0 , 5.0 ) title = plt . title ( \"Student t Distribution\" ) plt . savefig ( \"student-t.svg\" ) plt . close () Beta \u00b6 df = long_form_data ( \"pp.Beta(0.5, 0.5)\" , \"pp.Beta(5.0, 1.0)\" , \"pp.Beta(1.0, 3.0)\" , \"pp.Beta(2.0, 2.0)\" , \"pp.Beta(2.0, 5.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , element = \"step\" , fill = False ) xlim = plt . xlim ( 0.0 , 1.0 ) ylim = plt . ylim ( 0.0 , 4.0 ) title = plt . title ( \"Beta Distribution\" ) plt . savefig ( \"beta.svg\" ) plt . close ()","title":"Distributions"},{"location":"distributions/#distributions","text":"import pioupiou as pp import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns To streamline the visualization of distributions in this document, we introduce a helper function long_form_data . It instantiates some distributions, simulates them and returns the results as a long-form dataframe with column names \"Distribution\" and \"Value\" . Its arguments are: distribs : strings that should eval to random variables, n : number of samples used for the simulation (default: 100000 ) def long_form_data ( * distribs , n = 100000 ): # Modeling and Simulation pp . restart () Xs = [ eval ( distrib ) for distrib in distribs ] # random variables omega = pp . Omega ( n ) xs = [ X ( omega ) for X in Xs ] # Long-form Data Frame data = [] for distrib , x in zip ( distribs , xs ): data . extend ([[ distrib , np . float64 ( value )] for value in x ]) return pd . DataFrame ( data , columns = [ \"Distribution\" , \"Value\" ])","title":"Distributions"},{"location":"distributions/#bernoulli","text":"The snippet B = pp.Bernoulli(p) instantiates a random boolean variable \\(B\\) such that \\[ \\begin{array}{lcl} \\mathbb{P}(B = \\mathrm{true}) &=& p \\\\ \\mathbb{P}(B = \\mathrm{false}) &=& 1-p \\\\ \\end{array} \\] For example: >>> pp . restart () >>> B = pp . Bernoulli ( 0.5 ) >>> omega = pp . Omega ( 10 ) >>> b = B ( omega ) >>> b # doctest: +NORMALIZE_WHITESPACE array ([ False , True , True , True , False , False , False , False , False , False ]) The parameter p is optional; its default value is 0.5 . Thus B = Bernoulli() is equivalent to B = Bernoulli(0.5) . >>> pp . restart () >>> B = pp . Bernoulli () >>> omega = pp . Omega ( 10 ) >>> all ( b == B ( omega )) True With p=0.0 or p=1.0 you will get almost surely False and True respectively. >>> B = pp . Bernoulli ( 0.0 ) >>> omega = pp . Omega ( 10 ) >>> all ( B ( omega ) == False ) True >>> B = pp . Bernoulli ( 1.0 ) >>> omega = pp . Omega ( 10 ) >>> all ( B ( omega ) == True ) True With a larger number of independent samples, we can check these probabilities in a histogram df = long_form_data ( \"pp.Bernoulli(0.0)\" , \"pp.Bernoulli(0.25)\" , \"pp.Bernoulli(0.5)\" , \"pp.Bernoulli()\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"probability\" , common_norm = False , multiple = \"dodge\" , discrete = True , shrink = 0.5 ) yticks = plt . yticks ([ 0.0 , 0.25 , 0.5 , 0.75 , 1.0 ]) xticks = plt . xticks ([ 0 , 1 ], [ \"False\" , \"True\" ]) title = plt . title ( \"Bernoulli Distribution\" ) plt . savefig ( \"bernoulli.svg\" ) plt . close ()","title":"Bernoulli"},{"location":"distributions/#binomial","text":"When \\(n \\in \\mathbb{N}\\) and \\(p \\in [0,1]\\) , the code B = pp.Binomial(n, p) instantiates a random variable \\(B\\) with probability mass function $$ f(k) = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k} $$ for \\(k \\in \\{0,n\\}\\) and \\(f(k)=0\\) otherwise. The parameter \\(p\\) has a default value of \\(0.5\\) . df = long_form_data ( \"pp.Binomial(5)\" , \"pp.Binomial(5, 0.50)\" , \"pp.Binomial(5, 0.25)\" , \"pp.Binomial(5, 0.75)\" , ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"probability\" , common_norm = False , multiple = \"dodge\" , discrete = True , shrink = 0.5 ) yticks = plt . yticks ([ 0.0 , 0.125 , 0.25 , 0.375 , 0.5 ]) title = plt . title ( \"Binomial Distribution\" ) plt . savefig ( \"binomial.svg\" ) plt . close ()","title":"Binomial"},{"location":"distributions/#poisson","text":"The code pp.Poisson(lambda_) creates a random variable with probability mass function $$ f(k) = \\lambda^k \\frac{e^{-\\lambda}}{k!}, \\; k \\in \\mathbb{N} $$ and \\(f(k)=0\\) otherwise. The parameter \\(\\lambda\\) should be real and positive. df = long_form_data ( \"pp.Poisson(1.0)\" , \"pp.Poisson(2.0)\" , \"pp.Poisson(4.0)\" , \"pp.Binomial(100, 0.04)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"probability\" , common_norm = False , multiple = \"dodge\" , discrete = True , shrink = 0.8 ) xlim = plt . xlim ( - 1.0 , 11.0 ) title = plt . title ( \"Poisson Distribution\" ) plt . savefig ( \"poisson.svg\" ) plt . close ()","title":"Poisson"},{"location":"distributions/#uniform","text":"When a < b , the snippet U = pp.Uniform(a, b) creates a random variable \\(U\\) with density $$ f(x) = \\frac{1}{b-a} \\; \\mbox{ if } \\; a \\leq x \\leq b, $$ and \\(f(x)= 0\\) otherwise. The default value of a is 0.0 and the default value of b is 1.0, thus U = pp.Uniform() is equivalent to U = pp.Uniform(0,1) . For example >>> pp . restart () >>> U = pp . Uniform () >>> omega = pp . Omega () >>> U ( omega ) 0.6369616873214543 is equivalent to >>> pp . restart () >>> U = pp . Uniform ( 0.0 , 1.0 ) >>> omega = pp . Omega () >>> U ( omega ) 0.6369616873214543 We are almost sure that values sampled from U = pp.Uniform(a, b) are between a and b : >>> pp . restart () >>> a , b = - 3 , 7 >>> U = pp . Uniform ( a , b ) >>> omega = pp . Omega ( 1000 ) >>> all ( a <= U ( omega )) and all ( U ( omega ) <= b ) True Let's visualize some examples of the uniform distribution df = long_form_data ( \"pp.Uniform(-1.5, -1.0)\" , \"pp.Uniform( 0.0, 1.0)\" , \"pp.Uniform( 2.0, 4.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , bins = np . arange ( - 2.0 , 4.5 , 0.25 ), common_norm = False , ) xticks = plt . xticks ( np . arange ( - 2.0 , 4.5 , 1.0 )) title = plt . title ( \"Uniform Distribution\" ) plt . savefig ( \"uniform.svg\" ) plt . close ()","title":"Uniform"},{"location":"distributions/#normal","text":"The snippet N = pp.Normal(mu, sigma**2) creates a random variable \\(N\\) with density $$ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). $$ The default values of mu and sigma**2 are 0.0 and 1.0 : >>> pp . restart () >>> N = pp . Normal () >>> omega = pp . Omega () >>> N ( omega ) 0.3503492272565639 >>> pp . restart () >>> N = pp . Normal ( 0.0 , 1.0 ) >>> omega = pp . Omega () >>> N ( omega ) 0.3503492272565639 The parameters \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma > 0\\) are the mean and standard deviation of the random variable: >>> pp . restart () >>> N = pp . Normal ( 1.0 , ( 0.1 ) ** 2 ) >>> omega = pp . Omega ( 100000 ) >>> n = N ( omega ) >>> n # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array ([ 1.03503492 , 0.93865418 , 0.82605011 , ... , 1.00156987 ]) >>> np . mean ( n ) 0.9998490788460421 >>> np . std ( n ) 0.09990891658278829 Let's visualize some normal distributions: df = long_form_data ( \"pp.Normal( 0.0, 1.0)\" , \"pp.Normal(-2.0, 1.0)\" , \"pp.Normal( 2.0, 2.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , bins = [ - 1e9 ] + list ( np . linspace ( - 5 , 5 , 10 * 5 + 1 )) + [ 1e9 ], element = \"step\" ) xlim = plt . xlim ( - 5.0 , 5.0 ) title = plt . title ( \"Normal Distribution\" ) plt . savefig ( \"normal.svg\" ) plt . close ()","title":"Normal"},{"location":"distributions/#exponential","text":">>> pp.restart() >>> E = pp.Exponential() >>> omega = pp.Omega() >>> E(omega) 1.013246905717726 >>> pp.restart() >>> E = pp.Exponential(1.0) >>> omega = pp.Omega() >>> E(omega) 1.013246905717726 >>> pp.restart() >>> E = pp.Exponential(2.0) >>> omega = pp.Omega(1000) >>> np.mean(E(omega)) 0.5170714017411246 df = long_form_data ( \"pp.Exponential(0.5)\" , \"pp.Exponential(1.0)\" , \"pp.Exponential(2.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , element = \"step\" ) xlim = plt . xlim ( 0.0 , 5.0 ) title = plt . title ( \"Exponential Distribution\" ) plt . savefig ( \"exponential.svg\" ) plt . close ()","title":"Exponential"},{"location":"distributions/#cauchy","text":"Cauchy(x0=0.0, gamma=1.0) generates a random variable with density $$ f(x) = \\frac{1}{\\pi \\gamma} \\frac{\\gamma^2}{(x-x_0)^2 + \\gamma^2}. $$ >>> pp.restart() >>> C = pp.Cauchy() >>> omega = pp.Omega() >>> C(omega) 0.4589573340936978 >>> pp.restart() >>> C = pp.Cauchy(0.0, 1.0) >>> omega = pp.Omega() >>> C(omega) 0.4589573340936978 >>> pp.restart() >>> C = pp.Cauchy(3.0, 2.0) >>> omega = pp.Omega(1000) >>> np.median(C(omega)) 3.181434516919701 df = long_form_data ( \"pp.Cauchy( 0.0, 1.0)\" , \"pp.Cauchy(-2.0, 1.0)\" , \"pp.Cauchy( 2.0, 2.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , bins = [ - 1e9 ] + list ( np . linspace ( - 5 , 5 , 10 * 5 + 1 )) + [ 1e9 ], element = \"step\" ) xlim = plt . xlim ( - 5.0 , 5.0 ) title = plt . title ( \"Cauchy Distribution\" ) plt . savefig ( \"cauchy.svg\" ) plt . close ()","title":"Cauchy"},{"location":"distributions/#student","text":"df = long_form_data ( \"pp.t(0.1)\" , \"pp.t(1.0)\" , \"pp.t(10.0)\" , \"pp.Normal(0.0, 1.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , bins = [ - 1e9 ] + list ( np . linspace ( - 5 , 5 , 10 * 5 + 1 )) + [ 1e9 ], element = \"step\" , fill = False , ) xlim = plt . xlim ( - 5.0 , 5.0 ) title = plt . title ( \"Student t Distribution\" ) plt . savefig ( \"student-t.svg\" ) plt . close ()","title":"Student"},{"location":"distributions/#beta","text":"df = long_form_data ( \"pp.Beta(0.5, 0.5)\" , \"pp.Beta(5.0, 1.0)\" , \"pp.Beta(1.0, 3.0)\" , \"pp.Beta(2.0, 2.0)\" , \"pp.Beta(2.0, 5.0)\" ) ax = sns . histplot ( data = df , x = \"Value\" , hue = \"Distribution\" , stat = \"density\" , common_norm = False , element = \"step\" , fill = False ) xlim = plt . xlim ( 0.0 , 1.0 ) ylim = plt . ylim ( 0.0 , 4.0 ) title = plt . title ( \"Beta Distribution\" ) plt . savefig ( \"beta.svg\" ) plt . close ()","title":"Beta"},{"location":"gaussians/","text":"Gaussians \u00b6 Imports: import numpy as np import pioupiou as pp ; pp . restart () Bivariate Gaussian \u00b6 def Normal2 ( mu1 , mu2 , Sigma11 , Sigma12 , Sigma22 ): Sigma21 = Sigma12 N1 = pp . Normal ( mu1 , Sigma11 ) mu = mu2 + Sigma21 / Sigma11 * ( N1 - mu1 ) N2 = pp . Normal ( mu , Sigma22 - Sigma21 / Sigma22 * Sigma12 ) return N1 , N2 Model \u00b6 mu , Sigma = [ 0.0 , 0.0 ], [[ 1.0 , 0.75 ], [ 0.75 , 1.0 ]] X , Y = Normal2 ( mu [ 0 ], mu [ 1 ], Sigma [ 0 ][ 0 ], Sigma [ 0 ][ 1 ], Sigma [ 1 ][ 1 ]) Simulation \u00b6 >>> omega = pp . Omega ( 1000 ) >>> x , y = X ( omega ), Y ( omega ) >>> x # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array ([ 3.50349227e-01 , - 6.13458179e-01 , - 1.73949889e+00 , ... ]) >>> y # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array ([ - 1.20958718e+00 , 1.65204470e-01 , - 1.31085277e+00 , ... ]) Visualization \u00b6 import matplotlib.pyplot as plt import pandas as pd import seaborn as sns data = pd . DataFrame ({ \"x\" : x , \"y\" : y }) p = sns . jointplot ( x = \"x\" , y = \"y\" , data = data , kind = \"scatter\" , alpha = 1.0 , xlim = ( - 4.0 , 4.0 ), ylim = ( - 4.0 , 4.0 )) _ = p . fig . suptitle ( \"Correlated Gaussian Variables\" , fontsize = \"medium\" ) p . fig . tight_layout () plt . savefig ( \"gaussians.svg\" ) References \u00b6 Gaussians, in Advanced Robotics (Berkeley CS 287) by Pieter Abbeel","title":"Gaussians"},{"location":"gaussians/#gaussians","text":"Imports: import numpy as np import pioupiou as pp ; pp . restart ()","title":"Gaussians"},{"location":"gaussians/#bivariate-gaussian","text":"def Normal2 ( mu1 , mu2 , Sigma11 , Sigma12 , Sigma22 ): Sigma21 = Sigma12 N1 = pp . Normal ( mu1 , Sigma11 ) mu = mu2 + Sigma21 / Sigma11 * ( N1 - mu1 ) N2 = pp . Normal ( mu , Sigma22 - Sigma21 / Sigma22 * Sigma12 ) return N1 , N2","title":"Bivariate Gaussian"},{"location":"gaussians/#model","text":"mu , Sigma = [ 0.0 , 0.0 ], [[ 1.0 , 0.75 ], [ 0.75 , 1.0 ]] X , Y = Normal2 ( mu [ 0 ], mu [ 1 ], Sigma [ 0 ][ 0 ], Sigma [ 0 ][ 1 ], Sigma [ 1 ][ 1 ])","title":"Model"},{"location":"gaussians/#simulation","text":">>> omega = pp . Omega ( 1000 ) >>> x , y = X ( omega ), Y ( omega ) >>> x # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array ([ 3.50349227e-01 , - 6.13458179e-01 , - 1.73949889e+00 , ... ]) >>> y # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE array ([ - 1.20958718e+00 , 1.65204470e-01 , - 1.31085277e+00 , ... ])","title":"Simulation"},{"location":"gaussians/#visualization","text":"import matplotlib.pyplot as plt import pandas as pd import seaborn as sns data = pd . DataFrame ({ \"x\" : x , \"y\" : y }) p = sns . jointplot ( x = \"x\" , y = \"y\" , data = data , kind = \"scatter\" , alpha = 1.0 , xlim = ( - 4.0 , 4.0 ), ylim = ( - 4.0 , 4.0 )) _ = p . fig . suptitle ( \"Correlated Gaussian Variables\" , fontsize = \"medium\" ) p . fig . tight_layout () plt . savefig ( \"gaussians.svg\" )","title":"Visualization"},{"location":"gaussians/#references","text":"Gaussians, in Advanced Robotics (Berkeley CS 287) by Pieter Abbeel","title":"References"},{"location":"height/","text":"Height \u00b6 Imports: import numpy as np import pioupiou as pp ; pp . restart () Models \u00b6 Sex as a symbol (\u2640 or \u2642), for fun ! @pp . randomize @np . vectorize def sex_symbol ( is_female ): if is_female : return \"\u2640\" else : return \"\u2642\" Proportion of females in the population: proportion_female = 0.516 Is_female = pp . Bernoulli ( proportion_female ) Sex_symbol = sex_symbol ( Is_female ) Distribution of heights (sex-dependent): mu_female = 161.7 sigma_female = ( 175.0 - 149.0 ) / ( 2 * 1.96 ) Height_female = pp . Normal ( mu_female , sigma_female ** 2 ) mu_male = 174.4 sigma_male = ( 189.0 - 162.0 ) / ( 2 * 1.96 ) Height_male = pp . Normal ( mu_male , sigma_male ** 2 ) Combined height: Is_male = pp . logical_not ( Is_female ) Height = Is_female * Height_female + Is_male * Height_male Simulation \u00b6 Sexes: >>> omega = pp . Omega ( 1000 ) >>> Sex_symbol ( omega ) # doctest: +ELLIPSIS array ([ '\u2642' , '\u2640' , '\u2640' , '\u2640' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2640' , ... ) Height (sex-dependent): >>> Height_female ( omega ) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS array ([ 146.93582968 , 167.97025732 , 161.63754187 , 160.62997627 , ... ]) >>> Height_male ( omega ) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS array ([ 188.17949253 , 163.69348513 , 183.98172232 , 170.37550933 , ... ]) Combined height: >>> height = Height ( omega ) >>> height # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS array ([ 188.17949253 , 167.97025732 , 161.63754187 , 160.62997627 , ... ]) Visualization \u00b6 import matplotlib.pyplot as plt import pandas as pd import seaborn as sns df = pd . DataFrame ({ \"Height [cm]\" : height }) _ = sns . displot ( df , x = \"Height [cm]\" , stat = \"density\" , kde = True , aspect = 16 / 9 ) _ = plt . title ( \"Height Distribution in France\" ) plt . gcf () . subplots_adjust ( top = 0.95 ) plt . savefig ( \"height.svg\" ) References \u00b6 Anthropom\u00e9trie, Adultes : Tableaux de distribution ENNS (in French) France - Population, Female (% Of Total)","title":"Height"},{"location":"height/#height","text":"Imports: import numpy as np import pioupiou as pp ; pp . restart ()","title":"Height"},{"location":"height/#models","text":"Sex as a symbol (\u2640 or \u2642), for fun ! @pp . randomize @np . vectorize def sex_symbol ( is_female ): if is_female : return \"\u2640\" else : return \"\u2642\" Proportion of females in the population: proportion_female = 0.516 Is_female = pp . Bernoulli ( proportion_female ) Sex_symbol = sex_symbol ( Is_female ) Distribution of heights (sex-dependent): mu_female = 161.7 sigma_female = ( 175.0 - 149.0 ) / ( 2 * 1.96 ) Height_female = pp . Normal ( mu_female , sigma_female ** 2 ) mu_male = 174.4 sigma_male = ( 189.0 - 162.0 ) / ( 2 * 1.96 ) Height_male = pp . Normal ( mu_male , sigma_male ** 2 ) Combined height: Is_male = pp . logical_not ( Is_female ) Height = Is_female * Height_female + Is_male * Height_male","title":"Models"},{"location":"height/#simulation","text":"Sexes: >>> omega = pp . Omega ( 1000 ) >>> Sex_symbol ( omega ) # doctest: +ELLIPSIS array ([ '\u2642' , '\u2640' , '\u2640' , '\u2640' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2642' , '\u2640' , ... ) Height (sex-dependent): >>> Height_female ( omega ) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS array ([ 146.93582968 , 167.97025732 , 161.63754187 , 160.62997627 , ... ]) >>> Height_male ( omega ) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS array ([ 188.17949253 , 163.69348513 , 183.98172232 , 170.37550933 , ... ]) Combined height: >>> height = Height ( omega ) >>> height # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS array ([ 188.17949253 , 167.97025732 , 161.63754187 , 160.62997627 , ... ])","title":"Simulation"},{"location":"height/#visualization","text":"import matplotlib.pyplot as plt import pandas as pd import seaborn as sns df = pd . DataFrame ({ \"Height [cm]\" : height }) _ = sns . displot ( df , x = \"Height [cm]\" , stat = \"density\" , kde = True , aspect = 16 / 9 ) _ = plt . title ( \"Height Distribution in France\" ) plt . gcf () . subplots_adjust ( top = 0.95 ) plt . savefig ( \"height.svg\" )","title":"Visualization"},{"location":"height/#references","text":"Anthropom\u00e9trie, Adultes : Tableaux de distribution ENNS (in French) France - Population, Female (% Of Total)","title":"References"},{"location":"tests/","text":"Misc API Tests \u00b6 Warning: this document is a very rough draft. >>> from pioupiou import * >>> import pioupiou as pp >>> import numpy as np Random Variables \u00b6 The universe \u00b6 A random variable >>> restart() >>> U = Uniform(0.0, 1.0) >>> omega = Omega() >>> u = U(omega) >>> u 0.6369616873214543 >>> omega = Omega() >>> u = U(omega) >>> u 0.2697867137638703 The universe is literally the source of the randomness of every variable : sample Universe to get an omega and use it as an argument of a random variable. Once you have create a random variable, you can sample the universe to get an omega >>> restart() >>> U = Uniform(0.0, 1.0) >>> omega = Omega() >>> u = U(omega) >>> u 0.6369616873214543 >>> restart() >>> U1 = Uniform(0.0, 1.0) >>> omega = Omega(10) >>> for u1 in U1(omega): ... print(u1) 0.6369616873214543 0.2697867137638703 0.04097352393619469 0.016527635528529094 0.8132702392002724 0.9127555772777217 0.6066357757671799 0.7294965609839984 0.5436249914654229 0.9350724237877682 >>> restart() >>> U1 = Uniform(0.0, 1.0) >>> U2 = Uniform(0.0, U1) >>> omega = Omega(10) >>> for u2 in U2(omega): ... print(u2) 0.5196674564404565 0.0007388109615460543 0.03513087464975646 0.0005550901476646821 0.5934070594518621 0.16033064738516523 0.5236352151856017 0.39499409807791175 0.16293087393547157 0.395243164429411 >>> restart() >>> N = Normal(1.5, (2.7)**2) >>> ns = N(Omega(10000)) >>> print(\"mean:\", np.mean(ns)) mean: 1.4950310577047152 >>> print(\"std dev:\", np.std(ns)) std dev: 2.705182786283677 Constants \u00b6 >>> import pioupiou as pp; pp.restart() Constant distributions : >>> C = Constant(np.pi) >>> omega = Omega() >>> C(omega) 3.141592653589793 Yes, you can randomize a constant random variable ! \ud83d\ude00 This is a bit silly, but since we want to be able to randomize all distribution parameters, it is the most consistent choice. >>> X = pp.Uniform() >>> C = Constant(X) >>> omega = Omega() >>> X(omega) == C(omega) True Vectorization \u00b6 Pioupiou supports vectors (NumPy arrays) of (independent) samples : >>> pp.restart() A call to Omega without arguments generate a single sample omega . >>> X = pp.Uniform() >>> omega = pp.Omega() >>> X(omega) 0.6369616873214543 With an integer size instead, you get one-dimensional arrays of samples : >>> omega = pp.Omega(1) >>> X(omega) array([0.26978671]) >>> omega = pp.Omega(2) >>> X(omega) array([0.04097352, 0.01652764]) Arbitrary shapes are possible with a tuple size : >>> omega = pp.Omega((2,)) >>> X(omega) array([0.81327024, 0.91275558]) >>> omega = pp.Omega((2,3)) >>> X(omega) array([[0.60663578, 0.72949656, 0.54362499], [0.93507242, 0.81585355, 0.0027385 ]]) \ud83c\udf89 Randomize Everything! \u00b6 Constants, random variables, functions, etc. >>> pp.restart() The randomize function turns constant values into (constant) random variables : >>> c = 1.0 >>> C = randomize(1.0) >>> omega = Omega() >>> C(omega) == c True Obviously, randomizing a random variable doesn't do anything : >>> U = Uniform() >>> X = randomize(U) >>> omega = Omega() >>> X(omega) == U(omega) True Randomizing a function makes it able to take random variables as inputs ; it then produces a random variable : >>> @randomize ... def sum(x, c): ... return x + c >>> Y = sum(X, C) >>> omega = Omega() >>> Y(omega) == X(omega) + C(omega) True The randomized function can still accept fixed (non-random) values or even a mix of deterministic and random values : >>> sum(1.0, 2.0) 3.0 >>> Z = sum(X, 2.0) >>> omega = Omega() >>> Z(omega) == X(omega) + 2.0 True Randomization meets Vectorization \u00b6 If you want to keep pioupiou happy and working for you, only randomize functions that accept NumPy arrays (of consistent sizes). If your function doesn't do that by default, use the vectorize decorator provided by NumPy before you randomize them. \ud83d\udc25 Please ! >>> pp.restart() >>> X, Y = pp.Uniform(), pp.Uniform() So don't do >>> @pp.randomize ... def max(x, y): ... if x <= y: ... return y ... else: ... return x >>> Z = max(X, Y) unless you want to break everything : >>> omega = pp.Omega() >>> Z(omega) 0.6369616873214543 >>> omega = pp.Omega(10) >>> Z(omega) # doctest: +ELLIPSIS Traceback (most recent call last): ... ValueError: ... But instead do : >>> pp.restart() >>> X, Y = pp.Uniform(), pp.Uniform() >>> import numpy as np >>> @pp.randomize ... @np.vectorize ... def max(x, y): ... if x <= y: ... return y ... else: ... return x >>> Z = max(X, Y) It will work as expected : >>> omega = pp.Omega(10) >>> X(omega) array([0.63696169, 0.26978671, 0.04097352, 0.01652764, 0.81327024, 0.91275558, 0.60663578, 0.72949656, 0.54362499, 0.93507242]) >>> Y(omega) array([0.81585355, 0.0027385 , 0.85740428, 0.03358558, 0.72965545, 0.17565562, 0.86317892, 0.54146122, 0.29971189, 0.42268722]) >>> Z(omega) array([0.81585355, 0.26978671, 0.85740428, 0.03358558, 0.81327024, 0.91275558, 0.86317892, 0.72949656, 0.54362499, 0.93507242]) >>> all(max(X(omega), Y(omega)) == Z(omega)) True TODO: document somewhere that additional random variables can make the universe \"grow\" and make samples of omega obsolete. The safe way to proceed is to model EVERYTHING and then to sample (and of course, adding random variables that depend deterministically on random variables is OK too). TODO. After the randomization of functions that depends deterministically of their arguments, document the creation of components that inherit from RandomVariable and can grow the universe (do NOT merely depend on their arguments, but also on some hidden, random source).","title":"Tests"},{"location":"tests/#misc-api-tests","text":"Warning: this document is a very rough draft. >>> from pioupiou import * >>> import pioupiou as pp >>> import numpy as np","title":"Misc API Tests"},{"location":"tests/#random-variables","text":"","title":"Random Variables"},{"location":"tests/#the-universe","text":"A random variable >>> restart() >>> U = Uniform(0.0, 1.0) >>> omega = Omega() >>> u = U(omega) >>> u 0.6369616873214543 >>> omega = Omega() >>> u = U(omega) >>> u 0.2697867137638703 The universe is literally the source of the randomness of every variable : sample Universe to get an omega and use it as an argument of a random variable. Once you have create a random variable, you can sample the universe to get an omega >>> restart() >>> U = Uniform(0.0, 1.0) >>> omega = Omega() >>> u = U(omega) >>> u 0.6369616873214543 >>> restart() >>> U1 = Uniform(0.0, 1.0) >>> omega = Omega(10) >>> for u1 in U1(omega): ... print(u1) 0.6369616873214543 0.2697867137638703 0.04097352393619469 0.016527635528529094 0.8132702392002724 0.9127555772777217 0.6066357757671799 0.7294965609839984 0.5436249914654229 0.9350724237877682 >>> restart() >>> U1 = Uniform(0.0, 1.0) >>> U2 = Uniform(0.0, U1) >>> omega = Omega(10) >>> for u2 in U2(omega): ... print(u2) 0.5196674564404565 0.0007388109615460543 0.03513087464975646 0.0005550901476646821 0.5934070594518621 0.16033064738516523 0.5236352151856017 0.39499409807791175 0.16293087393547157 0.395243164429411 >>> restart() >>> N = Normal(1.5, (2.7)**2) >>> ns = N(Omega(10000)) >>> print(\"mean:\", np.mean(ns)) mean: 1.4950310577047152 >>> print(\"std dev:\", np.std(ns)) std dev: 2.705182786283677","title":"The universe"},{"location":"tests/#constants","text":">>> import pioupiou as pp; pp.restart() Constant distributions : >>> C = Constant(np.pi) >>> omega = Omega() >>> C(omega) 3.141592653589793 Yes, you can randomize a constant random variable ! \ud83d\ude00 This is a bit silly, but since we want to be able to randomize all distribution parameters, it is the most consistent choice. >>> X = pp.Uniform() >>> C = Constant(X) >>> omega = Omega() >>> X(omega) == C(omega) True","title":"Constants"},{"location":"tests/#vectorization","text":"Pioupiou supports vectors (NumPy arrays) of (independent) samples : >>> pp.restart() A call to Omega without arguments generate a single sample omega . >>> X = pp.Uniform() >>> omega = pp.Omega() >>> X(omega) 0.6369616873214543 With an integer size instead, you get one-dimensional arrays of samples : >>> omega = pp.Omega(1) >>> X(omega) array([0.26978671]) >>> omega = pp.Omega(2) >>> X(omega) array([0.04097352, 0.01652764]) Arbitrary shapes are possible with a tuple size : >>> omega = pp.Omega((2,)) >>> X(omega) array([0.81327024, 0.91275558]) >>> omega = pp.Omega((2,3)) >>> X(omega) array([[0.60663578, 0.72949656, 0.54362499], [0.93507242, 0.81585355, 0.0027385 ]])","title":"Vectorization"},{"location":"tests/#randomize-everything","text":"Constants, random variables, functions, etc. >>> pp.restart() The randomize function turns constant values into (constant) random variables : >>> c = 1.0 >>> C = randomize(1.0) >>> omega = Omega() >>> C(omega) == c True Obviously, randomizing a random variable doesn't do anything : >>> U = Uniform() >>> X = randomize(U) >>> omega = Omega() >>> X(omega) == U(omega) True Randomizing a function makes it able to take random variables as inputs ; it then produces a random variable : >>> @randomize ... def sum(x, c): ... return x + c >>> Y = sum(X, C) >>> omega = Omega() >>> Y(omega) == X(omega) + C(omega) True The randomized function can still accept fixed (non-random) values or even a mix of deterministic and random values : >>> sum(1.0, 2.0) 3.0 >>> Z = sum(X, 2.0) >>> omega = Omega() >>> Z(omega) == X(omega) + 2.0 True","title":"\ud83c\udf89 Randomize Everything!"},{"location":"tests/#randomization-meets-vectorization","text":"If you want to keep pioupiou happy and working for you, only randomize functions that accept NumPy arrays (of consistent sizes). If your function doesn't do that by default, use the vectorize decorator provided by NumPy before you randomize them. \ud83d\udc25 Please ! >>> pp.restart() >>> X, Y = pp.Uniform(), pp.Uniform() So don't do >>> @pp.randomize ... def max(x, y): ... if x <= y: ... return y ... else: ... return x >>> Z = max(X, Y) unless you want to break everything : >>> omega = pp.Omega() >>> Z(omega) 0.6369616873214543 >>> omega = pp.Omega(10) >>> Z(omega) # doctest: +ELLIPSIS Traceback (most recent call last): ... ValueError: ... But instead do : >>> pp.restart() >>> X, Y = pp.Uniform(), pp.Uniform() >>> import numpy as np >>> @pp.randomize ... @np.vectorize ... def max(x, y): ... if x <= y: ... return y ... else: ... return x >>> Z = max(X, Y) It will work as expected : >>> omega = pp.Omega(10) >>> X(omega) array([0.63696169, 0.26978671, 0.04097352, 0.01652764, 0.81327024, 0.91275558, 0.60663578, 0.72949656, 0.54362499, 0.93507242]) >>> Y(omega) array([0.81585355, 0.0027385 , 0.85740428, 0.03358558, 0.72965545, 0.17565562, 0.86317892, 0.54146122, 0.29971189, 0.42268722]) >>> Z(omega) array([0.81585355, 0.26978671, 0.85740428, 0.03358558, 0.81327024, 0.91275558, 0.86317892, 0.72949656, 0.54362499, 0.93507242]) >>> all(max(X(omega), Y(omega)) == Z(omega)) True TODO: document somewhere that additional random variables can make the universe \"grow\" and make samples of omega obsolete. The safe way to proceed is to model EVERYTHING and then to sample (and of course, adding random variables that depend deterministically on random variables is OK too). TODO. After the randomization of functions that depends deterministically of their arguments, document the creation of components that inherit from RandomVariable and can grow the universe (do NOT merely depend on their arguments, but also on some hidden, random source).","title":"Randomization meets Vectorization"},{"location":"universe/","text":"The Universe \u00b6 import pioupiou as pp The Big Bang \u00b6 You may have noticed that every time I define a new model, the first thing I do is: pp . restart () While this action is not mandatory, it serves two purposes: It restores the initial state of pioupou \ud83d\udc23. All the random variables that you have defined so far become invalid, but since you have reduced the size of your model (to nothing !), future sampling will be less computationally expensive. It ensures a deterministic sampling of your random variables. Import pioupiou, create your model and sample it; then restart pioupiou and do it again; you will end up with the same values 1 . Invalid Operations \u00b6 Every previously defined random variable becomes invalid when pioupiou is restarted. To avoid any mistake, pioupiou ensures that you cannot call any such variable. Consider the random variable U : pp . restart () U = pp . Uniform () It is perfectly valid and thus can be sampled: >>> omega = pp . Omega () >>> U ( omega ) 0.6369616873214543 But once pioupiou has been restarted, any attempt to sample U will raise an exception: >>> pp . restart () >>> omega = pp . Omega () >>> U ( omega ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... pioupiou . InvalidRandomVariable ... Similarly, if you generate a sample omega and then extend your model with a random variable that requires a larger universe (see Universe Structure ), using the sample afterwards will be an invalid operation: >>> pp . restart () >>> U1 = pp . Uniform () >>> omega = pp . Omega () >>> U2 = pp . Uniform () >>> U1 ( omega ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... pioupiou . InvalidSample ... >>> U2 ( omega ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... pioupiou . InvalidSample ... Warning New random variables do not always require a larger universe, so in some cases you can get away with using old samples (refer to the section Universe Structure for details). But you can remember that in any case, it is always safe to build completely your model (define all your random variables) before you start your sampling. Deterministic Sampling \u00b6 Let's see the deterministic sampling in action. A model is a collection of random variables: def make_model (): X = pp . Uniform ( 0 , 1 ) Y = pp . Uniform ( 0 , 1 ) Z = X + Y return X , Y , Z The first run gives us >>> pp . restart () >>> X , Y , Z = make_model () >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ), Z ( omega ) ( 0.6369616873214543 , 0.2697867137638703 , 0.9067484010853246 ) Without a restart, new modeling and sampling steps will (probably) give different results: >>> X , Y , Z = make_model () >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ), Z ( omega ) ( 0.8132702392002724 , 0.9127555772777217 , 1.726025816477994 ) But if we restart pioupiou and recreate the model, we have reproduced the original results: >>> pp . restart () >>> X , Y , Z = make_model () >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ), Z ( omega ) ( 0.6369616873214543 , 0.2697867137638703 , 0.9067484010853246 ) Universe Structure \u00b6 Internals This section explains the structure of omega in omega = pp.Omega() . But this is an implementation detail: you can treat omega as an opaque object and merely use it to sample your random variables. In pioupou, all randomness is derived from \\(n\\) primitive random variables which are independent and uniformly distributed on \\([0,1]\\) . Concretely, that means that every random variable in your model depends deterministically on these \\(n\\) primitive random variables 2 . The number \\(n\\) itself depends on the complexity of your model: every time that you invoke pp.Uniform() (directly or indirectly), you instantiate a new primitive random variable. The call pp.Omega() merely samples these \\(n\\) primitive random variables 3 . Let's see how that works. When no model has been defined, we obviously need zero primitive random variables, thus \\(n=0\\) and omega = pp.Omega() is array of length 0: >>> pp . restart () >>> pp . Omega . n 0 >>> omega = pp . Omega () >>> omega array ([], dtype = float64 ) If we create a new uniform random variable on \\([0,1]\\) now \\(n\\) is 1. >>> U = pp . Uniform () >>> pp . Omega . n 1 >>> omega = pp . Omega () >>> omega array ([ 0.63696169 ]) Guess what? Here \\(U\\) is exactly the first (and only) primitive random variable: >>> U ( omega ) 0.6369616873214543 >>> U ( omega ) == omega [ 0 ] True Since internally, each call to pp.Normal() instantiate a new (independant) uniform variable on \\([0, 1]\\) , adding an independent normal variable to the model will increase the number of primitive random variables by one: >>> N = pp . Normal () >>> pp . Omega . n 2 >>> omega = pp . Omega () >>> omega array ([ 0.26978671 , 0.04097352 ]) As usual, this \\(\\omega\\) can be used to sample the random variables \\(U\\) and \\(N\\) : >>> U ( omega ), N ( omega ) ( 0.2697867137638703 , - 1.739498886765934 ) Note that if you add to your model a random variable that depends deterministically on the existing ones, you won't increase the number of primitive random variables. >>> X = U + N >>> pp . Omega . n 2 >>> omega = pp . Omega () >>> omega array ([ 0.01652764 , 0.81327024 ]) >>> U ( omega ), N ( omega ), X ( omega ) ( 0.016527635528529094 , 0.8900118529686626 , 0.9065394884971917 ) As a special case, constant random variables \u2013 which depend deterministically on zero existing random variables - do not increase the size of the universe either: >>> I = pp . Constant ( 1.0 ) >>> pp . Omega . n 2 >>> omega = pp . Omega () >>> omega array ([ 0.91275558 , 0.60663578 ]) >>> U ( omega ), N ( omega ), X ( omega ), I ( omega ) ( 0.9127555772777217 , 0.2705613202510434 , 1.1833168975287651 , 1.0 ) You may not appreciate this feature but it is terrific when you are testing models since your execution is repeatable. \u21a9 Your universe is \\(\\Omega = [0,1]^n\\) and its probability \\(\\mathbb{P}\\) is the Lebesgue measure ; for any measurable set \\(A \\subset \\Omega\\) , $$ \\mathbb{P}(A) = \\int_{\\Omega} 1_A(\\omega) \\, d\\omega. $$ Every random variable is a (measurable) function \\(X :\\Omega \\to \\mathbb{R}\\) . What we call primitive random variables in this context are the \\(n\\) random variables \\(U_1, \\dots, U_n\\) defined by \\(U_i(\\omega_1, \\dots, \\omega_n) = \\omega_i\\) . Thus, for any random variable \\(X\\) , we have $$ X(\\omega_1, \\dots, \\omega_n) = X(U_1(\\omega_1, \\dots, \\omega_n), \\dots, U_n(\\omega_1, \\dots, \\omega_n)). $$ This proves that any random variable \\(X\\) in this universe depends deterministically on \\(U_1, \\dots, U_n\\) . \u21a9 Or if you wish, samples the universe \\(\\Omega\\) . \u21a9","title":"The Universe"},{"location":"universe/#the-universe","text":"import pioupiou as pp","title":"The Universe"},{"location":"universe/#the-big-bang","text":"You may have noticed that every time I define a new model, the first thing I do is: pp . restart () While this action is not mandatory, it serves two purposes: It restores the initial state of pioupou \ud83d\udc23. All the random variables that you have defined so far become invalid, but since you have reduced the size of your model (to nothing !), future sampling will be less computationally expensive. It ensures a deterministic sampling of your random variables. Import pioupiou, create your model and sample it; then restart pioupiou and do it again; you will end up with the same values 1 .","title":"The Big Bang"},{"location":"universe/#invalid-operations","text":"Every previously defined random variable becomes invalid when pioupiou is restarted. To avoid any mistake, pioupiou ensures that you cannot call any such variable. Consider the random variable U : pp . restart () U = pp . Uniform () It is perfectly valid and thus can be sampled: >>> omega = pp . Omega () >>> U ( omega ) 0.6369616873214543 But once pioupiou has been restarted, any attempt to sample U will raise an exception: >>> pp . restart () >>> omega = pp . Omega () >>> U ( omega ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... pioupiou . InvalidRandomVariable ... Similarly, if you generate a sample omega and then extend your model with a random variable that requires a larger universe (see Universe Structure ), using the sample afterwards will be an invalid operation: >>> pp . restart () >>> U1 = pp . Uniform () >>> omega = pp . Omega () >>> U2 = pp . Uniform () >>> U1 ( omega ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... pioupiou . InvalidSample ... >>> U2 ( omega ) # doctest: +ELLIPSIS Traceback ( most recent call last ): ... pioupiou . InvalidSample ... Warning New random variables do not always require a larger universe, so in some cases you can get away with using old samples (refer to the section Universe Structure for details). But you can remember that in any case, it is always safe to build completely your model (define all your random variables) before you start your sampling.","title":"Invalid Operations"},{"location":"universe/#deterministic-sampling","text":"Let's see the deterministic sampling in action. A model is a collection of random variables: def make_model (): X = pp . Uniform ( 0 , 1 ) Y = pp . Uniform ( 0 , 1 ) Z = X + Y return X , Y , Z The first run gives us >>> pp . restart () >>> X , Y , Z = make_model () >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ), Z ( omega ) ( 0.6369616873214543 , 0.2697867137638703 , 0.9067484010853246 ) Without a restart, new modeling and sampling steps will (probably) give different results: >>> X , Y , Z = make_model () >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ), Z ( omega ) ( 0.8132702392002724 , 0.9127555772777217 , 1.726025816477994 ) But if we restart pioupiou and recreate the model, we have reproduced the original results: >>> pp . restart () >>> X , Y , Z = make_model () >>> omega = pp . Omega () >>> X ( omega ), Y ( omega ), Z ( omega ) ( 0.6369616873214543 , 0.2697867137638703 , 0.9067484010853246 )","title":"Deterministic Sampling"},{"location":"universe/#universe-structure","text":"Internals This section explains the structure of omega in omega = pp.Omega() . But this is an implementation detail: you can treat omega as an opaque object and merely use it to sample your random variables. In pioupou, all randomness is derived from \\(n\\) primitive random variables which are independent and uniformly distributed on \\([0,1]\\) . Concretely, that means that every random variable in your model depends deterministically on these \\(n\\) primitive random variables 2 . The number \\(n\\) itself depends on the complexity of your model: every time that you invoke pp.Uniform() (directly or indirectly), you instantiate a new primitive random variable. The call pp.Omega() merely samples these \\(n\\) primitive random variables 3 . Let's see how that works. When no model has been defined, we obviously need zero primitive random variables, thus \\(n=0\\) and omega = pp.Omega() is array of length 0: >>> pp . restart () >>> pp . Omega . n 0 >>> omega = pp . Omega () >>> omega array ([], dtype = float64 ) If we create a new uniform random variable on \\([0,1]\\) now \\(n\\) is 1. >>> U = pp . Uniform () >>> pp . Omega . n 1 >>> omega = pp . Omega () >>> omega array ([ 0.63696169 ]) Guess what? Here \\(U\\) is exactly the first (and only) primitive random variable: >>> U ( omega ) 0.6369616873214543 >>> U ( omega ) == omega [ 0 ] True Since internally, each call to pp.Normal() instantiate a new (independant) uniform variable on \\([0, 1]\\) , adding an independent normal variable to the model will increase the number of primitive random variables by one: >>> N = pp . Normal () >>> pp . Omega . n 2 >>> omega = pp . Omega () >>> omega array ([ 0.26978671 , 0.04097352 ]) As usual, this \\(\\omega\\) can be used to sample the random variables \\(U\\) and \\(N\\) : >>> U ( omega ), N ( omega ) ( 0.2697867137638703 , - 1.739498886765934 ) Note that if you add to your model a random variable that depends deterministically on the existing ones, you won't increase the number of primitive random variables. >>> X = U + N >>> pp . Omega . n 2 >>> omega = pp . Omega () >>> omega array ([ 0.01652764 , 0.81327024 ]) >>> U ( omega ), N ( omega ), X ( omega ) ( 0.016527635528529094 , 0.8900118529686626 , 0.9065394884971917 ) As a special case, constant random variables \u2013 which depend deterministically on zero existing random variables - do not increase the size of the universe either: >>> I = pp . Constant ( 1.0 ) >>> pp . Omega . n 2 >>> omega = pp . Omega () >>> omega array ([ 0.91275558 , 0.60663578 ]) >>> U ( omega ), N ( omega ), X ( omega ), I ( omega ) ( 0.9127555772777217 , 0.2705613202510434 , 1.1833168975287651 , 1.0 ) You may not appreciate this feature but it is terrific when you are testing models since your execution is repeatable. \u21a9 Your universe is \\(\\Omega = [0,1]^n\\) and its probability \\(\\mathbb{P}\\) is the Lebesgue measure ; for any measurable set \\(A \\subset \\Omega\\) , $$ \\mathbb{P}(A) = \\int_{\\Omega} 1_A(\\omega) \\, d\\omega. $$ Every random variable is a (measurable) function \\(X :\\Omega \\to \\mathbb{R}\\) . What we call primitive random variables in this context are the \\(n\\) random variables \\(U_1, \\dots, U_n\\) defined by \\(U_i(\\omega_1, \\dots, \\omega_n) = \\omega_i\\) . Thus, for any random variable \\(X\\) , we have $$ X(\\omega_1, \\dots, \\omega_n) = X(U_1(\\omega_1, \\dots, \\omega_n), \\dots, U_n(\\omega_1, \\dots, \\omega_n)). $$ This proves that any random variable \\(X\\) in this universe depends deterministically on \\(U_1, \\dots, U_n\\) . \u21a9 Or if you wish, samples the universe \\(\\Omega\\) . \u21a9","title":"Universe Structure"},{"location":"volatility/","text":"Stochastic Volatility \u00b6 Imports: import numpy as np import pioupiou as pp ; pp . restart () Model \u00b6 The evolution of the asset price \\(Y_t\\) at time \\(t\\) is given by \\[ Y_t \\sim \\mathcal{N}(0, (\\exp(H_t/2))^2) \\] where initially \\[ H_0 \\sim \\mathcal{N}\\left(\\mu, \\left(\\frac{\\sigma}{\\sqrt{1 - \\phi^2}}\\right)^2\\right) \\] and then \\[ H_t = \\mu + \\phi(H_{t-1}- \\mu) + \\Delta_t \\; \\mbox{ with } \\; \\Delta_t \\sim \\mathcal{N}(0,\\sigma^2). \\] mu , phi , sigma = - 1.02 , 0.95 , 0.25 n = 100 ts = np . arange ( n ) H , Y = np . zeros ( n , dtype = object ), np . zeros ( n , dtype = object ) for t in ts : if t == 0 : H [ t ] = pp . Normal ( mu , sigma ** 2 / ( 1 - phi * phi )) else : H [ t ] = mu + phi * ( H [ t - 1 ] - mu ) + pp . Normal ( 0 , sigma ** 2 ) Y [ t ] = pp . Normal ( 0 , pp . exp ( 0.5 * H [ t ]) ** 2 ) Simulation \u00b6 Given that our model is deeply nested, we need to increase the recursion limit: import sys sys . setrecursionlimit ( 10000 ) Then, as usual: >>> omega = pp . Omega ( 100 ) >>> y = np . array ([ Yt ( omega ) for Yt in Y ]) >>> y array ([[ - 0.03467247 , - 0.34340423 , 0.25390125 , ... , 0.96859772 , 1.98091379 , 0.19754246 ], [ 0.81756718 , 0.08954963 , - 0.57286052 , ... , - 2.04860967 , - 0.18998793 , - 0.25094848 ], [ - 0.81096151 , 0.60424628 , 0.44987883 , ... , - 0.8988355 , 1.18628077 , - 0.664733 ], ... , [ - 0.01718594 , - 0.5785354 , - 0.39955856 , ... , - 1.4898044 , - 0.59212422 , 0.22604821 ], [ 0.08299539 , 0.98580976 , - 1.09395039 , ... , 1.61315771 , 1.12770729 , - 0.44484522 ], [ 0.40431131 , - 0.68520976 , 0.4213367 , ... , 0.52895462 , 0.75351984 , 0.12524886 ]]) Visualization \u00b6 import matplotlib.pyplot as plt import pandas as pd import seaborn as sns data = { \"trajectory\" : [], \"Time\" : [], \"Price\" : [], \"type\" : []} for t in np . arange ( len ( y )): for omega in np . arange ( np . shape ( y )[ 1 ]): data [ \"trajectory\" ] . append ( omega ) data [ \"Time\" ] . append ( t ) data [ \"Price\" ] . append ( y [ t , omega ]) data [ \"type\" ] . append ( \"aggregate\" ) data = pd . DataFrame . from_dict ( data ) _ = sns . lineplot ( data = data , x = \"Time\" , y = \"Price\" , markers = True , dashes = False , style = \"type\" , hue = \"type\" ) _ = plt . title ( \"Evolution of the Asset Price\" ) plt . gcf () . subplots_adjust ( left = 0.20 , top = 0.90 ) plt . savefig ( \"volatility.svg\" ) References \u00b6 Stochastic Volatility Models in Stan User's Guide","title":"Volatility"},{"location":"volatility/#stochastic-volatility","text":"Imports: import numpy as np import pioupiou as pp ; pp . restart ()","title":"Stochastic Volatility"},{"location":"volatility/#model","text":"The evolution of the asset price \\(Y_t\\) at time \\(t\\) is given by \\[ Y_t \\sim \\mathcal{N}(0, (\\exp(H_t/2))^2) \\] where initially \\[ H_0 \\sim \\mathcal{N}\\left(\\mu, \\left(\\frac{\\sigma}{\\sqrt{1 - \\phi^2}}\\right)^2\\right) \\] and then \\[ H_t = \\mu + \\phi(H_{t-1}- \\mu) + \\Delta_t \\; \\mbox{ with } \\; \\Delta_t \\sim \\mathcal{N}(0,\\sigma^2). \\] mu , phi , sigma = - 1.02 , 0.95 , 0.25 n = 100 ts = np . arange ( n ) H , Y = np . zeros ( n , dtype = object ), np . zeros ( n , dtype = object ) for t in ts : if t == 0 : H [ t ] = pp . Normal ( mu , sigma ** 2 / ( 1 - phi * phi )) else : H [ t ] = mu + phi * ( H [ t - 1 ] - mu ) + pp . Normal ( 0 , sigma ** 2 ) Y [ t ] = pp . Normal ( 0 , pp . exp ( 0.5 * H [ t ]) ** 2 )","title":"Model"},{"location":"volatility/#simulation","text":"Given that our model is deeply nested, we need to increase the recursion limit: import sys sys . setrecursionlimit ( 10000 ) Then, as usual: >>> omega = pp . Omega ( 100 ) >>> y = np . array ([ Yt ( omega ) for Yt in Y ]) >>> y array ([[ - 0.03467247 , - 0.34340423 , 0.25390125 , ... , 0.96859772 , 1.98091379 , 0.19754246 ], [ 0.81756718 , 0.08954963 , - 0.57286052 , ... , - 2.04860967 , - 0.18998793 , - 0.25094848 ], [ - 0.81096151 , 0.60424628 , 0.44987883 , ... , - 0.8988355 , 1.18628077 , - 0.664733 ], ... , [ - 0.01718594 , - 0.5785354 , - 0.39955856 , ... , - 1.4898044 , - 0.59212422 , 0.22604821 ], [ 0.08299539 , 0.98580976 , - 1.09395039 , ... , 1.61315771 , 1.12770729 , - 0.44484522 ], [ 0.40431131 , - 0.68520976 , 0.4213367 , ... , 0.52895462 , 0.75351984 , 0.12524886 ]])","title":"Simulation"},{"location":"volatility/#visualization","text":"import matplotlib.pyplot as plt import pandas as pd import seaborn as sns data = { \"trajectory\" : [], \"Time\" : [], \"Price\" : [], \"type\" : []} for t in np . arange ( len ( y )): for omega in np . arange ( np . shape ( y )[ 1 ]): data [ \"trajectory\" ] . append ( omega ) data [ \"Time\" ] . append ( t ) data [ \"Price\" ] . append ( y [ t , omega ]) data [ \"type\" ] . append ( \"aggregate\" ) data = pd . DataFrame . from_dict ( data ) _ = sns . lineplot ( data = data , x = \"Time\" , y = \"Price\" , markers = True , dashes = False , style = \"type\" , hue = \"type\" ) _ = plt . title ( \"Evolution of the Asset Price\" ) plt . gcf () . subplots_adjust ( left = 0.20 , top = 0.90 ) plt . savefig ( \"volatility.svg\" )","title":"Visualization"},{"location":"volatility/#references","text":"Stochastic Volatility Models in Stan User's Guide","title":"References"}]}